{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0_5l2aE3sM5J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "69fb5abe-aea7-4d72-ff03-e6985cd80929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For J:\n",
            "Training accuracy:\n",
            "80.16\n",
            "Test accuracy:\n",
            "81.26\n",
            "#################################\n",
            "#################################\n",
            "For L:\n",
            "Training accuracy:\n",
            "92.01\n",
            "Test accuracy:\n",
            "91.84\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "## Pre-process training and test datasets\t\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_test = x_test.reshape(10000,784)\n",
        "\n",
        "\n",
        "\n",
        "k = 10\n",
        "\n",
        "##################### Softmax Function ##################################\n",
        "\n",
        "def softmax(theta):\n",
        "  # op = []\n",
        "  # sum = 0\n",
        "  # for j in range(len(theta)):\n",
        "  #   sum = sum + np.exp(theta[j])\n",
        "  # for i in range(len(theta)):\n",
        "  #   nr = np.exp(theta[i])\n",
        "  #   op.append(nr/sum) \n",
        "  y = np.exp(theta)\n",
        "  y = y / np.expand_dims(np.sum(y, axis=1), axis=1)\n",
        "\n",
        "  return y\n",
        "\n",
        "\n",
        "###################### One hot encodig function #########################\n",
        "def one_hot_encode(y,cla):\n",
        "  ohe = np.zeros((y.shape[0],cla))\n",
        "  for i,ys in enumerate(y):\n",
        "    ohe[i][ys] = 1\n",
        "  return ohe\n",
        "\n",
        "\n",
        "\n",
        "##################### Batch Gradient Descent ############################\n",
        "\n",
        "d = x_train.shape[1]\n",
        "n = x_train.shape[0]\n",
        "wt = np.zeros(d)\n",
        "W = np.array([wt]*k)\n",
        "W = W.T\n",
        "y_one = one_hot_encode(y_train,k)\n",
        "\n",
        "converge = 0\n",
        "\n",
        "eta = 0.001\n",
        "\n",
        "t=0\n",
        "tt=[]\n",
        "eee=[]\n",
        "################# Gradient descent for J #################################\n",
        "\n",
        "### Replace convergence with L1 norm #########################\n",
        "while converge != d:\n",
        "\n",
        "\n",
        "  # Calculate Gradients for J\n",
        "  GG1 = np.dot(x_train.T,(y_one - (np.dot(x_train , W))))/n\n",
        "  W = W + eta * GG1\n",
        "\n",
        "  w_old = wt\n",
        "\n",
        "\n",
        "  GG2 = np.dot(x_train.T,(y_train - (np.dot(x_train, wt))))/n\n",
        "  wt = wt + (eta * GG2)\n",
        "\n",
        "  cost1 = 0.5*(np.linalg.norm((y_train - np.dot(x_train,wt)),ord=2)**2) / n\n",
        "  #print(cost1)\n",
        "\n",
        "\n",
        "  # y1 = np.dot(x_train,W)\n",
        "  # y2 = np.argmax(y1,axis=1)\n",
        "  # err_train = 0\n",
        "  # for i in range(n):\n",
        "  #   if y2[i] != y_train[i]:\n",
        "  #     err_train += 1\n",
        "  # eee.append(err_train)\n",
        "  # print(err_train*100/n)\n",
        "  # t=t+1\n",
        "  # tt.append(t)\n",
        "\n",
        "\n",
        "  #Checking for convergence\n",
        "  converge = 0\n",
        "  for i in range(len(wt)):\n",
        "    if np.absolute(w_old[i] - wt[i]) <= 0.0001:\n",
        "      converge = converge + 1\n",
        "\n",
        "\n",
        "################ Calculate Accuracy correctly for J #####################\n",
        "y1 = np.dot(x_train,W)\n",
        "\n",
        "y2 = np.argmax(y1,axis=1)\n",
        "\n",
        "\n",
        "n_test = len(y_test)\n",
        "y1_test = np.dot(x_test,W)\n",
        "y2_test = np.argmax(y1_test,axis=1)\n",
        "YY_test = np.dot(x_test,wt)\n",
        "\n",
        "err_train = 0\n",
        "for i in range(n):\n",
        "  if y2[i] != y_train[i]:\n",
        "    err_train += 1\n",
        "print(\"For J:\")\n",
        "print(\"Training accuracy:\")\n",
        "print(100 - (err_train*100/n))\n",
        "\n",
        "err_test = 0\n",
        "for i in range(n_test):\n",
        "  if y2_test[i] != y_test[i]:\n",
        "    err_test += 1\n",
        "\n",
        "print(\"Test accuracy:\")\n",
        "print(100 - (err_test*100/n_test))\n",
        "\n",
        "################################################################\n",
        "\n",
        "################### Gradient Descent for L ###################\n",
        "wt = np.zeros(d)\n",
        "W_l = np.array([wt]*k)\n",
        "W_l = W_l.T\n",
        "y_hat = np.exp(np.dot(x_train, W_l))\n",
        "y_hat = y_hat / np.expand_dims(np.sum(y_hat, axis=1), axis=1)\n",
        "\n",
        "GG_cap =  np.dot(x_train.T, y_one - y_hat)\n",
        "eta = 0.0001\n",
        "# delta = 5e-4 \n",
        "# while converge != d:\n",
        "T = False\n",
        "while np.amax(np.abs(GG_cap)) > 0.1 and T == False:\n",
        "\n",
        "\n",
        "  w_old1 = W_l\n",
        "\n",
        "  y_cap = softmax(np.dot(x_train,W_l))\n",
        "\n",
        "\n",
        "  GG_cap = np.dot(x_train.T, y_one - y_cap)\n",
        "\n",
        "  W_l = W_l + eta * GG_cap\n",
        "  \n",
        "  y1 = np.dot(x_train,W_l)\n",
        "  y2 = np.argmax(y1,axis=1)\n",
        "  err_train = 0\n",
        "  for i in range(n):\n",
        "    if y2[i] != y_train[i]:\n",
        "      err_train += 1\n",
        "  eee.append(err_train)\n",
        "  # print(err_train*100/n)\n",
        "  err_train = err_train*100/n\n",
        "  if err_train < 8:\n",
        "    T = True\n",
        "  t=t+1\n",
        "  tt.append(t)\n",
        "\n",
        "  #Checking for convergence\n",
        "  # converge = 0\n",
        "  # for i in range(len(W_l)):\n",
        "  #  # if np.absolute(w_old1[i] - W_l[i]) <= 0.001:\n",
        "  #   if np.argmax(W_l[i]) == np.argmax(w_old):\n",
        "  #     converge = converge + 1\n",
        "\n",
        "################ Calculate error percentage for J ##################\n",
        "\n",
        "\n",
        "print(\"#################################\")\n",
        "print(\"#################################\")\n",
        "\n",
        "################ Calculate error percentage for L ##############\n",
        "\n",
        "y1 = np.dot(x_train,W_l)\n",
        "\n",
        "y2 = np.argmax(y1,axis=1)\n",
        "\n",
        "\n",
        "n_test = len(y_test)\n",
        "y1_test = np.dot(x_test,W_l)\n",
        "y2_test = np.argmax(y1_test,axis=1)\n",
        "# YY_test = np.dot(x_test,wt)\n",
        "\n",
        "err_train = 0\n",
        "for i in range(n):\n",
        "  if y2[i] != y_train[i]:\n",
        "    err_train += 1\n",
        "print(\"For L:\")\n",
        "print(\"Training accuracy:\")\n",
        "print(100 - (err_train*100/n))\n",
        "\n",
        "err_test = 0\n",
        "for i in range(n_test):\n",
        "  if y2_test[i] != y_test[i]:\n",
        "    err_test += 1\n",
        "\n",
        "print(\"Test accuracy:\")\n",
        "print(100 - (err_test*100/n_test))"
      ]
    }
  ]
}